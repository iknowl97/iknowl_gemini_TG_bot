import asyncio
import logging
import os
import tempfile  # For temporary files
import pathlib   # For path operations
import csv
from datetime import datetime

from dotenv import load_dotenv

from aiogram import Bot, Dispatcher, Router, F
from aiogram.filters import CommandStart, Command
from aiogram.types import Message, Voice, PhotoSize, Document, Video, Audio, Sticker, Contact, Location
from aiogram.enums import ParseMode, ChatAction
from aiogram.client.default import DefaultBotProperties

# For Gemini API
import google.generativeai as genai
from google.generativeai.types import generation_types

# For RAG
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFaceHub

# Load environment variables from .env file
load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME")
HUGGING_FACE_API_KEY = os.getenv("HUGGING_FACE_API_KEY")
HUGGING_FACE_MODEL = os.getenv("HUGGING_FACE_MODEL", "google/gemma-2b-it")

# Check for required tokens
if not BOT_TOKEN:
    raise ValueError("You must set the BOT_TOKEN environment variable.")
if not GEMINI_API_KEY:
    logging.warning(
        "GEMINI_API_KEY not set. Image/Audio/Document processing might be limited.")
if not MODEL_NAME:
    logging.warning(
        "MODEL_NAME not set. Image/Audio/Document processing might be limited.")
if not HUGGING_FACE_API_KEY:
    raise ValueError(
        "You must set the HUGGING_FACE_API_KEY environment variable for RAG.")

# Load system prompts from markdown files
PROMPT_DIR = pathlib.Path(__file__).parent / "prompts"


def load_prompt(filename):
    with open(PROMPT_DIR / filename, encoding="utf-8") as f:
        # skip the first line (title)
        return f.read().strip().split('\n', 1)[-1].strip()


TEXT_SYSTEM_PROMPT = load_prompt("text_system_prompt.md")
AUDIO_SYSTEM_PROMPT = load_prompt("audio_system_prompt.md")
IMAGE_SYSTEM_PROMPT = load_prompt("image_system_prompt.md")
# Load help and features text from markdown
HELP_TEXT = load_prompt("help_text.md")
FEATURES_TEXT = load_prompt("features_text.md")

# Gemini API setup
try:
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel(str(MODEL_NAME))
except Exception as e:
    logging.error(f"Gemini API configuration error: {e}")
    gemini_model = None

# RAG Setup
# Use a common embedding model
embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)

# Initialize Chroma with persistence (optional, removes need to re-embed each time)
# For simplicity, we'll use in-memory for now. Change to persistent later if needed.
# db_directory = "./chroma_db"
# vectorstore = Chroma(persist_directory=db_directory, embedding_function=embeddings)
vectorstore = Chroma(embedding_function=embeddings)

# Hugging Face LLM setup
hf_llm = HuggingFaceHub(
    repo_id=HUGGING_FACE_MODEL,
    task="text-generation",
    huggingfacehub_api_token=HUGGING_FACE_API_KEY,
)

# Create RetrievalQA chain (basic setup)
qa_chain = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="stuff",  # Stuffing all retrieved documents into the prompt
    retriever=vectorstore.as_retriever()
)

router = Router()

# /start command handler


@router.message(CommandStart())
async def cmd_start(message: Message):
    user_full_name = getattr(
        getattr(message, 'from_user', None), 'full_name', 'áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ”áƒšáƒ˜')
    await message.answer(
        f"áƒ’áƒáƒ›áƒáƒ áƒ¯áƒáƒ‘áƒ, {user_full_name}!\n"
        "áƒ›áƒ” áƒ•áƒáƒ  áƒ¨áƒ”áƒœáƒ˜ AI áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜. ğŸ¤–\n"
        "áƒ¨áƒ”áƒ’áƒ˜áƒ«áƒšáƒ˜áƒ áƒ’áƒáƒ›áƒáƒ›áƒ˜áƒ’áƒ–áƒáƒ•áƒœáƒ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜, áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ áƒáƒœ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜!\n"
        "áƒ§áƒ•áƒ”áƒšáƒ AI áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ˜áƒ¥áƒœáƒ”áƒ‘áƒ áƒ—áƒáƒœáƒáƒ›áƒ”áƒ“áƒ áƒáƒ•áƒ”, áƒ’áƒáƒ›áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜áƒ—."
    )

# /help command handler


@router.message(Command("help"))
async def cmd_help(message: Message):
    await message.answer(HELP_TEXT)
    log_conversation(message.from_user.id, getattr(
        message.from_user, 'username', ''), "/help", HELP_TEXT)

# Provide help/features info on user request (text/voice)
HELP_KEYWORDS = ["áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ", "help", "áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ",
                 "áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ”áƒ‘áƒšáƒáƒ‘áƒ", "features", "áƒ áƒáƒ¡ áƒáƒ™áƒ”áƒ—áƒ”áƒ‘", "áƒ áƒ áƒ¨áƒ”áƒ’áƒ˜áƒ«áƒšáƒ˜áƒ"]


def is_help_request(text):
    return any(word in text.lower() for word in HELP_KEYWORDS)

# Helper to extract text from message (including caption, forwarded, etc.)


def extract_message_text(message: Message) -> str:
    parts = []
    if message.forward_from or message.forward_from_chat:
        parts.append("[Forwarded message]")
    if message.text:
        parts.append(message.text)
    if message.caption:
        parts.append(message.caption)
    return "\n".join(parts).strip()

# Generic handler for text, captions, and forwarded messages


@router.message(F.text | F.caption)
async def handle_text_and_caption_message(message: Message, bot: Bot):
    await bot.send_chat_action(message.chat.id, ChatAction.TYPING)
    user_text = extract_message_text(message)

    # Check if the message is a help/features request
    if is_help_request(user_text):
        await message.answer(HELP_TEXT)
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), user_text, "Sent help text")
        return  # Stop processing if it's a help request

    if not gemini_model:
        await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ— áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ. áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒ›áƒáƒ’áƒ•áƒ˜áƒáƒœáƒ”áƒ‘áƒ˜áƒ—. ğŸ˜”")
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), message.text, "AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ")
        return

    if not user_text:
        await message.answer("áƒ’áƒ—áƒ®áƒáƒ•áƒ—, áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒáƒœ áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒ”áƒ— áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ áƒáƒ¦áƒ¬áƒ”áƒ áƒ˜áƒ—. âœï¸")
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), user_text, "áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡")
        return
    processing_message = await message.answer("áƒ•áƒáƒ–áƒ áƒáƒ•áƒœáƒ”áƒ‘... ï¿½ï¿½")
    try:
        # Use RAG chain to get response
        # The qa_chain internally handles retrieval and generation
        response = await qa_chain.invoke({"query": user_text})

        await processing_message.delete()

        # The response from RetrievalQA is a dictionary, the answer is in the 'result' key
        if response and 'result' in response and response['result']:
            bot_response_text = response['result']
            await message.answer(bot_response_text)
            log_conversation(message.from_user.id, getattr(
                message.from_user, 'username', ''), user_text, bot_response_text)
        else:
            # Handle cases where RAG chain returns no result
            logging.warning(
                f"RAG chain returned no result for query: {user_text}")
            await message.answer("áƒ¡áƒáƒ›áƒ¬áƒ£áƒ®áƒáƒ áƒáƒ“, áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ•áƒ«áƒ”áƒšáƒ˜ áƒ—áƒ¥áƒ•áƒ”áƒœáƒ¡ áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ–áƒ” áƒáƒáƒ¡áƒ£áƒ®áƒ˜áƒ¡ áƒ’áƒáƒªáƒ”áƒ›áƒ áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ—. ğŸ˜”")
            log_conversation(message.from_user.id, getattr(
                message.from_user, 'username', ''), user_text, "RAG chain returned no result")

    except Exception as e:
        await processing_message.delete()
        logging.error(f"Error in RAG chain processing: {e}", exc_info=True)
        await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ. ğŸ˜µâ€ğŸ’«")
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), user_text, "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ RAG áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡")

# Enhanced image handler: supports photo and document with image MIME type


@router.message(F.photo | (F.document & (F.document.mime_type.startswith('image/'))))
async def handle_image_message(message: Message, bot: Bot):
    await bot.send_chat_action(message.chat.id, ChatAction.TYPING)
    if not gemini_model:
        await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ— áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡. ğŸ˜”")
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), message.text, "AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ")
        return
    # Get file info
    file_id = None
    file_unique_id = None
    if message.photo:
        photo = message.photo[-1]
        file_id = photo.file_id
        file_unique_id = photo.file_unique_id
        ext = 'jpg'
    elif message.document:
        file_id = message.document.file_id
        file_unique_id = message.document.file_unique_id
        ext = message.document.file_name.split(
            '.')[-1] if message.document.file_name else 'img'
    else:
        await message.answer("áƒ›áƒáƒ—áƒ®áƒáƒ•áƒœáƒáƒ¨áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ.")
        return
    processing_message = await message.answer("áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜ áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡... ğŸ–¼ï¸ğŸ‘€")
    gemini_file_resource = None
    with tempfile.TemporaryDirectory() as temp_dir_name:
        temp_dir_path = pathlib.Path(temp_dir_name)
        local_img_path = temp_dir_path / f"{file_unique_id}.{ext}"
        try:
            await bot.download(file=file_id, destination=local_img_path)
            if local_img_path.stat().st_size == 0:
                await processing_message.edit_text("Failed to download the image or the file is empty. ğŸ˜¥")
                log_conversation(message.from_user.id, getattr(
                    message.from_user, 'username', ''), message.text, "áƒ¡áƒ£áƒ áƒáƒ—áƒ˜/áƒ¤áƒáƒ˜áƒšáƒ˜ áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜áƒ")
                return
            gemini_file_resource = await asyncio.to_thread(
                genai.upload_file,
                path=local_img_path,
                display_name=f"image_message_{file_unique_id}.{ext}",
                mime_type=f"image/{ext}" if ext in ['jpg', 'jpeg',
                                                    'png', 'gif', 'bmp', 'webp'] else 'image/jpeg'
            )
            # Combine caption if present
            caption = extract_message_text(message)
            contents_for_gemini = [IMAGE_SYSTEM_PROMPT]
            if caption:
                contents_for_gemini.append(f"Caption: {caption}")
            contents_for_gemini.append(gemini_file_resource)
            response = await gemini_model.generate_content_async(contents_for_gemini)
            await processing_message.delete()
            if response.text:
                await message.answer(response.text)
                log_conversation(message.from_user.id, getattr(
                    message.from_user, 'username', ''), message.text, response.text)
            else:
                # Handle cases where the main AI response is empty
                logging.warning(
                    f"Gemini API returned an empty response for image: {file_id}")
                safety_feedback_info = ""
                if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                    safety_feedback_info += f"\nReason (prompt_feedback): {response.prompt_feedback}"
                if hasattr(response, 'candidates') and response.candidates:
                    for i, candidate in enumerate(response.candidates):
                        if hasattr(candidate, 'finish_reason'):
                            safety_feedback_info += f"\nCandidate {i} (finish_reason): {candidate.finish_reason}"
                        if hasattr(candidate, 'safety_ratings'):
                            safety_feedback_info += f"\nCandidate {i} (safety_ratings): {candidate.safety_ratings}"
                logging.warning(safety_feedback_info)
                await message.answer(f"áƒ¡áƒáƒ›áƒ¬áƒ£áƒ®áƒáƒ áƒáƒ“, áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ•áƒ«áƒ”áƒšáƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒáƒ¦áƒ¬áƒ”áƒ áƒ. ğŸ–¼ï¸âŒ áƒ›áƒáƒ¡ áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ›áƒáƒ®áƒ“áƒ”áƒ¡, áƒ áƒáƒ› áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ áƒáƒ  áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ’áƒáƒœáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒœ áƒ¬áƒ”áƒ¡áƒ”áƒ‘áƒ¡ áƒ¨áƒ”áƒ”áƒ áƒ¬áƒ§áƒ›áƒ.{safety_feedback_info if safety_feedback_info else ''}")
                log_conversation(message.from_user.id, getattr(
                    message.from_user, 'username', ''), message.text, "áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒáƒ¦áƒ¬áƒ”áƒ áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ®áƒ”áƒ áƒ®áƒ“áƒ")
        except Exception as e:
            await processing_message.delete()
            await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ. ğŸ˜µâ€ğŸ’«")
            log_conversation(message.from_user.id, getattr(
                message.from_user, 'username', ''), message.text, "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡")
        finally:
            if gemini_file_resource and hasattr(gemini_file_resource, 'name'):
                try:
                    await asyncio.to_thread(genai.delete_file, name=gemini_file_resource.name)
                except Exception:
                    pass  # Silent failure on file deletion is acceptable

# Enhanced document/file handler (non-image)


@router.message(F.document & ~(F.document.mime_type.startswith('image/')))
async def handle_document_message(message: Message, bot: Bot):
    await bot.send_chat_action(message.chat.id, ChatAction.TYPING)
    if not gemini_model:
        await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ— áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡. ğŸ˜”")
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), message.text, "AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ")
        return
    file_id = message.document.file_id
    file_unique_id = message.document.file_unique_id
    file_name = message.document.file_name or 'file'
    processing_message = await message.answer(f"áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ '{file_name}' áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ... ğŸ“„")
    gemini_file_resource = None
    with tempfile.TemporaryDirectory() as temp_dir_name:
        temp_dir_path = pathlib.Path(temp_dir_name)
        local_file_path = temp_dir_path / file_name
        try:
            await bot.download(file=file_id, destination=local_file_path)
            if local_file_path.stat().st_size == 0:
                await processing_message.edit_text("Failed to download the file or the file is empty. ğŸ˜¥")
                log_conversation(message.from_user.id, getattr(
                    message.from_user, 'username', ''), message.text, "áƒ¤áƒáƒ˜áƒšáƒ˜ áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜áƒ")
                return
            gemini_file_resource = await asyncio.to_thread(
                genai.upload_file,
                path=local_file_path,
                display_name=file_name,
                mime_type=message.document.mime_type or 'application/octet-stream'
            )
            caption = extract_message_text(message)
            contents_for_gemini = [
                f"You have received a file. Analyze and summarize its content in modern, literate Georgian. If a caption is present, use it for context.",
            ]
            if caption:
                contents_for_gemini.append(f"Caption: {caption}")
            contents_for_gemini.append(gemini_file_resource)
            response = await gemini_model.generate_content_async(contents_for_gemini)
            await processing_message.delete()
            if response.text:
                await message.answer(response.text)
                log_conversation(message.from_user.id, getattr(
                    message.from_user, 'username', ''), message.text, response.text)
            else:
                await message.answer("áƒ¡áƒáƒ›áƒ¬áƒ£áƒ®áƒáƒ áƒáƒ“, áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ•áƒ«áƒ”áƒšáƒ˜ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ. ğŸ“„âŒ")
        except Exception as e:
            await processing_message.delete()
            await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ. ğŸ˜µâ€ğŸ’«")
        finally:
            if gemini_file_resource and hasattr(gemini_file_resource, 'name'):
                try:
                    await asyncio.to_thread(genai.delete_file, name=gemini_file_resource.name)
                except Exception:
                    pass

# Voice message handler


@router.message(F.voice)
async def handle_voice_message(message: Message, bot: Bot):
    await bot.send_chat_action(message.chat.id, ChatAction.TYPING)
    if not gemini_model:
        await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ— áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ áƒáƒ£áƒ“áƒ˜áƒáƒ¡áƒ—áƒ•áƒ˜áƒ¡. ğŸ˜”")
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), message.text, "AI áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ")
        return
    voice = getattr(message, 'voice', None)
    if voice is None:
        await message.answer("áƒ›áƒáƒ—áƒ®áƒáƒ•áƒœáƒáƒ¨áƒ˜ áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ.")
        log_conversation(message.from_user.id, getattr(
            message.from_user, 'username', ''), message.text, "áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ")
        return
    processing_message = await message.answer("áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ... ğŸ¤ğŸ§")
    gemini_file_resource = None
    with tempfile.TemporaryDirectory() as temp_dir_name:
        temp_dir_path = pathlib.Path(temp_dir_name)
        local_ogg_path = temp_dir_path / f"{voice.file_unique_id}.ogg"
        try:
            await bot.download(file=voice.file_id, destination=local_ogg_path)
            if local_ogg_path.stat().st_size == 0:
                await processing_message.edit_text("áƒáƒ£áƒ“áƒ˜áƒ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ©áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ®áƒ”áƒ áƒ®áƒ“áƒ áƒáƒœ áƒ¤áƒáƒ˜áƒšáƒ˜ áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜áƒ. ğŸ˜¥")
                log_conversation(message.from_user.id, getattr(
                    message.from_user, 'username', ''), message.text, "áƒáƒ£áƒ“áƒ˜áƒ áƒ¤áƒáƒ˜áƒšáƒ˜ áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜áƒ")
                return
            # Step 1: Transcribe the audio using Gemini
            gemini_file_resource = await asyncio.to_thread(
                genai.upload_file,
                path=local_ogg_path,
                display_name=f"voice_message_{voice.file_unique_id}.ogg",
                mime_type="audio/ogg"
            )
            # Step 2: Ask Gemini to transcribe only (Georgian, monospace)
            transcription_prompt = (
                "Transcribe this audio to modern, literate Georgian. "
                "Return only the transcription, no explanation."
            )
            transcription_response = await gemini_model.generate_content_async([
                transcription_prompt,
                gemini_file_resource
            ])
            transcription = (transcription_response.text or "").strip()
            # Step 3: Double-check/correct the transcription
            verify_prompt = (
                "Check the following Georgian transcription for accuracy and correct any errors. "
                "Return only the improved transcription, no explanation."
            )
            verify_response = await gemini_model.generate_content_async(
                f"{verify_prompt}\n\nTranscription: {transcription}"
            )
            verified_transcription = (
                verify_response.text or transcription).strip()
            # Step 4: Send the verified transcription to the user in monospace/code format
            if verified_transcription:
                await message.answer(f"<code>{verified_transcription}</code>", parse_mode="HTML")
            # Step 5: Generate the final AI reply as before
            contents_for_gemini = [AUDIO_SYSTEM_PROMPT, gemini_file_resource]
            response = await gemini_model.generate_content_async(contents_for_gemini)
            await processing_message.delete()
            if response.text:
                await message.answer(response.text)
                log_conversation(message.from_user.id, getattr(
                    message.from_user, 'username', ''), verified_transcription, response.text)
            else:
                # Handle cases where the main AI response is empty
                logging.warning(
                    f"Gemini API returned an empty response for audio: {voice.file_id}")
                safety_feedback_info = ""
                if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                    safety_feedback_info += f"\nReason (prompt_feedback): {response.prompt_feedback}"
                if hasattr(response, 'candidates') and response.candidates:
                    for i, candidate in enumerate(response.candidates):
                        if hasattr(candidate, 'finish_reason'):
                            safety_feedback_info += f"\nCandidate {i} (finish_reason): {candidate.finish_reason}"
                        if hasattr(candidate, 'safety_ratings'):
                            safety_feedback_info += f"\nCandidate {i} (safety_ratings): {candidate.safety_ratings}"
                logging.warning(safety_feedback_info)
                await message.answer(f"áƒ¡áƒáƒ›áƒ¬áƒ£áƒ®áƒáƒ áƒáƒ“, áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ•áƒ«áƒ”áƒšáƒ˜ áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ. ğŸ¤âŒ áƒ›áƒáƒ¡ áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ›áƒáƒ®áƒ“áƒ”áƒ¡, áƒ áƒáƒ› áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ áƒáƒ  áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ’áƒáƒœáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒœ áƒ¬áƒ”áƒ¡áƒ”áƒ‘áƒ¡ áƒ¨áƒ”áƒ”áƒ áƒ¬áƒ§áƒ›áƒ.{safety_feedback_info if safety_feedback_info else ''}")
                log_conversation(message.from_user.id, getattr(message.from_user, 'username', ''), verified_transcription,
                                 f"áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ•áƒ”áƒ  áƒ’áƒ”áƒœáƒ”áƒ áƒ˜áƒ áƒ“áƒ. áƒ¡áƒáƒ›áƒ¬áƒ£áƒ®áƒáƒ áƒáƒ“, áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ•áƒ«áƒ”áƒšáƒ˜ áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ. áƒ›áƒáƒ¡ áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ›áƒáƒ®áƒ“áƒ”áƒ¡, áƒ áƒáƒ› áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ áƒáƒ  áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ’áƒáƒœáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒœ áƒ¬áƒ”áƒ¡áƒ”áƒ‘áƒ¡ áƒ¨áƒ”áƒ”áƒ áƒ¬áƒ§áƒ›áƒ.")
        except Exception as e:
            await processing_message.delete()
            await message.answer("áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ. ğŸ˜µâ€ğŸ’« áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒ›áƒáƒ’áƒ•áƒ˜áƒáƒœáƒ”áƒ‘áƒ˜áƒ—.")
            log_conversation(message.from_user.id, getattr(
                message.from_user, 'username', ''), message.text, "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ®áƒ›áƒáƒ•áƒáƒœáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡")
        finally:
            if gemini_file_resource and hasattr(gemini_file_resource, 'name'):
                try:
                    await asyncio.to_thread(genai.delete_file, name=gemini_file_resource.name)
                except Exception:
                    pass  # Silent failure on file deletion is acceptable

# Video handler


@router.message(F.video)
async def handle_video_message(message: Message, bot: Bot):
    await message.answer("áƒ•áƒ˜áƒ“áƒ”áƒ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜ áƒ¯áƒ”áƒ  áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ˜áƒšáƒ˜, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ”áƒ¡ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ áƒ›áƒáƒšáƒ” áƒ“áƒáƒ”áƒ›áƒáƒ¢áƒ”áƒ‘áƒ. ğŸ¬")

# Audio handler


@router.message(F.audio)
async def handle_audio_message(message: Message, bot: Bot):
    await message.answer("áƒáƒ£áƒ“áƒ˜áƒ áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜ áƒ¯áƒ”áƒ  áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ˜áƒšáƒ˜, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ”áƒ¡ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ áƒ›áƒáƒšáƒ” áƒ“áƒáƒ”áƒ›áƒáƒ¢áƒ”áƒ‘áƒ. ğŸµ")

# Sticker handler


@router.message(F.sticker)
async def handle_sticker_message(message: Message):
    await message.answer("áƒ¡áƒ¢áƒ˜áƒ™áƒ”áƒ áƒ”áƒ‘áƒ˜ áƒ¡áƒáƒ®áƒáƒšáƒ˜áƒ¡áƒáƒ! ğŸ˜„ áƒ—áƒ£áƒ›áƒªáƒ, áƒ¡áƒ¢áƒ˜áƒ™áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜ áƒ¯áƒ”áƒ  áƒáƒ  áƒ¨áƒ”áƒ›áƒ˜áƒ«áƒšáƒ˜áƒ.")

# Contact handler


@router.message(F.contact)
async def handle_contact_message(message: Message):
    await message.answer("áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒšáƒ˜áƒ áƒ™áƒáƒœáƒ¢áƒáƒ¥áƒ¢áƒ˜. áƒ™áƒáƒœáƒ¢áƒáƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ áƒ¯áƒ”áƒ  áƒáƒ  áƒ¨áƒ”áƒ›áƒ˜áƒ«áƒšáƒ˜áƒ, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ¡áƒ®áƒ•áƒ áƒ áƒáƒ›áƒ”áƒ–áƒ” áƒ—áƒ£ áƒ’áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒáƒ— áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ, áƒ›áƒáƒ›áƒ¬áƒ”áƒ áƒ”áƒ—!")

# Location handler


@router.message(F.location)
async def handle_location_message(message: Message):
    await message.answer("áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒšáƒ˜áƒ áƒšáƒáƒ™áƒáƒªáƒ˜áƒ. áƒšáƒáƒ™áƒáƒªáƒ˜áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ áƒ¯áƒ”áƒ  áƒáƒ  áƒ¨áƒ”áƒ›áƒ˜áƒ«áƒšáƒ˜áƒ, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ¡áƒ®áƒ•áƒ áƒ áƒáƒ›áƒ”áƒ–áƒ” áƒ—áƒ£ áƒ’áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒáƒ— áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ, áƒ›áƒáƒ›áƒ¬áƒ”áƒ áƒ”áƒ—!")

# Log conversation
LOG_FILE = pathlib.Path(__file__).parent / "user_conversations.csv"


def log_conversation(user_id, username, message, response):
    with open(LOG_FILE, "a", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            datetime.now().isoformat(),
            user_id,
            username,
            message.replace("\n", " "),
            response.replace("\n", " ") if response else ""
        ])

# Function to load data from CSV and populate Chroma


def load_conversations_to_chroma(file_path: pathlib.Path):
    if not file_path.exists():
        logging.warning(
            f"Conversation log file not found at {file_path}. No data loaded to Chroma.")
        return
    try:
        # Using CSVLoader from Langchain
        loader = CSVLoader(file_path=str(file_path))
        documents = loader.load()

        if not documents:
            logging.info("No documents loaded from conversation log.")
            return

        # Add documents to Chroma
        # Note: This will re-add documents every time the bot starts if using in-memory Chroma.
        # For persistent Chroma, you'd check if the collection exists and is populated.
        vectorstore.add_documents(documents)
        logging.info(
            f"Loaded {len(documents)} documents into Chroma from {file_path}.")

    except Exception as e:
        logging.error(f"Error loading conversations to Chroma: {e}")


async def main():
    default_properties = DefaultBotProperties(parse_mode=ParseMode.HTML)
    bot = Bot(token=str(BOT_TOKEN), default=default_properties)
    dp = Dispatcher()

    dp.include_router(router)

    # Load conversation data into Chroma on startup
    load_conversations_to_chroma(LOG_FILE)

    await bot.delete_webhook(drop_pending_updates=True)

    logging.info("Bot is starting...")
    try:
        await dp.start_polling(bot)
    finally:
        await bot.session.close()
        logging.info("Bot stopped.")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(levelname)s - %(message)s")
    asyncio.run(main())
